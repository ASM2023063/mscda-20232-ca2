{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05274a1a",
   "metadata": {},
   "source": [
    "# Title\n",
    "\n",
    "Continuous Assessment 2 10/11/2023 - BD & ADA Module // MSc in Data Analytics Y1 S2 - Student ID 2023063\n",
    "\n",
    "Data: ProjectsTweets.csv\n",
    "\n",
    "Github: https://github.com/ASM2023063/mscda-20232-ca2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2367fc5d",
   "metadata": {},
   "source": [
    "### 1. Research Understanding Phase \n",
    "\n",
    "\n",
    "### Draft rewrite description later\n",
    "- In this study an overarching analysis of Customer behavior in eCommerce shop was provided, preparing the customers’ event data for modeling and analysis to predict the number of purchases future customers will do.\n",
    "- PySparkSQL and TensorFlow were used in Jupyter Notebook file to complete this task, using SQL and Python coding language.\n",
    "- To perform an organized and clear understanding of the study timeline, Cross Industry Standard Process for Data Mining (CRISP-DM) Methodology was used, and the cells of code were grouped according to the methodology’s phases.\n",
    "- An Artificial Neural Network model was applied to the chosen variables X and y. For the evaluation phase Mean Absolute Error (MAE) and Mean Squared Error (MSE) were registered to measure the error between predicted and actual values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179e9fc",
   "metadata": {},
   "source": [
    "### 2. Data Understanding Phase\n",
    "\n",
    "Practical Big Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60dadde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import PySparkSQL\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession        \n",
    "\n",
    "# Create SparkSession with enableHiveSupport\n",
    "df = (SparkSession\n",
    "  .builder\n",
    "  .master(\"local[*]\")\n",
    "  .appName(\"SparkSQL\")\n",
    "  .enableHiveSupport() \n",
    "  .getOrCreate())\n",
    "\n",
    "# Path to dataset\n",
    "csv_file = \"file:////home/hduser/Downloads/2023063_CA2/ProjectTweets.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e45df9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read and create a temporary view\n",
    "df = (spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"false\")\n",
    "  .load(csv_file)\n",
    "  .toDF(\"id\",\"number\",\"date\",\"query\",\"name\",\"body\"))\n",
    "df.createOrReplaceTempView(\"temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "266ed1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- number: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualise inferred schema\n",
    "data = spark.sql(\"SELECT * FROM temp_view\")\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e93cac4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "| id|    number|                date|   query|           name|                body|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display 5 first rows\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b217dd69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 14:07:35,544 WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "2023-10-22 14:07:35,544 WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "2023-10-22 14:07:36,934 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "2023-10-22 14:07:36,934 WARN metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hduser@127.0.1.1\n",
      "2023-10-22 14:07:37,150 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "2023-10-22 14:07:37,161 ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Database projectdb already exists)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:925)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n",
      "\tat com.sun.proxy.$Proxy23.create_database(Unknown Source)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:725)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
      "\tat com.sun.proxy.$Proxy24.createDatabase(Unknown Source)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:434)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createDatabase$1(HiveClientImpl.scala:347)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:345)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createDatabase$1(HiveExternalCatalog.scala:193)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:193)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createDatabase(ExternalCatalogWithListener.scala:47)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:251)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:83)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS projectdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff038b83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 14:07:37,233 WARN analysis.ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "2023-10-22 14:07:37,663 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "2023-10-22 14:07:37,876 WARN conf.HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "2023-10-22 14:07:37,877 WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "2023-10-22 14:07:37,877 WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "2023-10-22 14:07:37,926 ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Table tweetstable already exists)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1416)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1503)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n",
      "\tat com.sun.proxy.$Proxy23.create_table_with_environment_context(Unknown Source)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
      "\tat com.sun.proxy.$Proxy24.createTable(Unknown Source)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:555)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:553)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:287)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:376)\n",
      "\tat org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:167)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table in database\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS projectdb.tweetsTable (id Int, number long, date String, query String, name String, body String)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "893c6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alocate data to hive database\n",
    "#spark.sql(\"INSERT INTO TABLE projectdb.tweetsTable SELECT * FROM temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "651eead9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------------------+--------+---------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id    |number    |date                        |query   |name           |body                                                                                                                                     |\n",
      "+------+----------+----------------------------+--------+---------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|816210|1551363506|Sat Apr 18 08:51:40 PDT 2009|NO_QUERY|prosario_2000  |@ctribe I hope you are having a great day.                                                                                               |\n",
      "|816211|1551363569|Sat Apr 18 08:51:39 PDT 2009|NO_QUERY|Chelsea_Volturi|@Boy_Kill_Boy Nope Just Bored Well Say That Most Of The Time The Usual                                                                   |\n",
      "|816212|1551363682|Sat Apr 18 08:51:41 PDT 2009|NO_QUERY|askbillmitchell|@marty0518 Sometimes? and just a little cryptic? LOL! I am just messing with you.  You are a good sport.                                 |\n",
      "|816213|1551363752|Sat Apr 18 08:51:41 PDT 2009|NO_QUERY|kendiixd       |so i guesss im not in coolifornia anymore how exiting                                                                                    |\n",
      "|816214|1551363844|Sat Apr 18 08:51:42 PDT 2009|NO_QUERY|ladycalypso    |@DaiLS I do that, too, but right now, it's the Radiant Dawn Soundtrack.                                                                  |\n",
      "|816215|1551363866|Sat Apr 18 08:51:43 PDT 2009|NO_QUERY|FindingAnswers |trendy topic - Record Store Day - just becuz a song has the word &quot;Lollipop&quot; in it, doesn't mean it's appropriate for children. |\n",
      "|816216|1551363911|Sat Apr 18 08:51:43 PDT 2009|NO_QUERY|HTwashere      |@firsttiger Real phone? i just read your blog on phones - they are not phones anymore                                                    |\n",
      "|816217|1551363926|Sat Apr 18 08:51:43 PDT 2009|NO_QUERY|kelliekano     |@Dragoncade I see you're delivering your daily dose of sunshine to the twitterverse!  Happy Saturday to ya...                            |\n",
      "|816218|1551363992|Sat Apr 18 08:51:44 PDT 2009|NO_QUERY|JoshMetz       |Hi Guys i'm new on here  Just wonder what I can use this for? I were trying/hoping that I could find the real Miley Cyrus                |\n",
      "|816219|1551364008|Sat Apr 18 08:51:44 PDT 2009|NO_QUERY|jeewillikers   |leaving for my hair appt!!!!                                                                                                             |\n",
      "+------+----------+----------------------------+--------+---------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display table content\n",
    "spark.sql(\"SELECT * FROM projectdb.tweetsTable\").show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c34cb740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:=============================>                             (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  total|\n",
      "+-------+\n",
      "|1600000|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Number of rows\n",
    "spark.sql(\"SELECT count(*) as total FROM projectdb.tweetsTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c421ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|count(DISTINCT id, number, date, query, name, body)|\n",
      "+---------------------------------------------------+\n",
      "|                                            1600000|\n",
      "+---------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:=============================>                             (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Number of unique rows\n",
    "df2 = spark.sql(\"SELECT count(distinct *) FROM projectdb.tweetsTable\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d5568",
   "metadata": {},
   "source": [
    "### 3. Data Preparation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79f60bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql.functions import split, to_date, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe6b8afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable \n",
    "tweet_data = spark.sql(\"SELECT * FROM projectdb.tweetsTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb6d2379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split column date into new columns\n",
    "tweet_data = tweet_data.withColumn(\"date_parts\", split(tweet_data[\"date\"],\" \"))\n",
    "\n",
    "tweet_data = tweet_data.withColumn(\"month\", tweet_data[\"date_parts\"][1])\n",
    "tweet_data = tweet_data.withColumn(\"day\", tweet_data[\"date_parts\"][2])\n",
    "tweet_data = tweet_data.withColumn(\"time\", tweet_data[\"date_parts\"][3])\n",
    "tweet_data = tweet_data.withColumn(\"year\", tweet_data[\"date_parts\"][5])\n",
    "tweet_data = tweet_data.withColumn(\"weekday\", tweet_data[\"date_parts\"][0])\n",
    "tweet_data = tweet_data.withColumn(\"timezone\", tweet_data[\"date_parts\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "331f122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split new column time into new columns\n",
    "tweet_data = tweet_data.withColumn(\"time_format\", split(tweet_data[\"time\"],\":\"))\n",
    "\n",
    "tweet_data = tweet_data.withColumn(\"hour\", tweet_data[\"time_format\"][0])\n",
    "tweet_data = tweet_data.withColumn(\"minute\", tweet_data[\"time_format\"][1])\n",
    "tweet_data = tweet_data.withColumn(\"second\", tweet_data[\"time_format\"][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18747be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "tweet_data = tweet_data.drop(\"date_parts\")\n",
    "tweet_data = tweet_data.drop(\"date\")\n",
    "tweet_data = tweet_data.drop(\"number\")\n",
    "tweet_data = tweet_data.drop(\"query\")\n",
    "tweet_data = tweet_data.drop(\"time_format\")\n",
    "tweet_data = tweet_data.drop(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da051b05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+--------------------+-----+---+----+-------+--------+----+------+------+\n",
      "|    id|           name|                body|month|day|year|weekday|timezone|hour|minute|second|\n",
      "+------+---------------+--------------------+-----+---+----+-------+--------+----+------+------+\n",
      "|816210|  prosario_2000|@ctribe I hope yo...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    40|\n",
      "|816211|Chelsea_Volturi|@Boy_Kill_Boy Nop...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    39|\n",
      "|816212|askbillmitchell|@marty0518 Someti...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    41|\n",
      "|816213|       kendiixd|so i guesss im no...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    41|\n",
      "|816214|    ladycalypso|@DaiLS I do that,...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    42|\n",
      "|816215| FindingAnswers|trendy topic - Re...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    43|\n",
      "|816216|      HTwashere|@firsttiger Real ...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    43|\n",
      "|816217|     kelliekano|@Dragoncade I see...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    43|\n",
      "|816218|       JoshMetz|Hi Guys i'm new o...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    44|\n",
      "|816219|   jeewillikers|leaving for my ha...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    44|\n",
      "|816220|     reesypants|@reedoh Hello lov...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    44|\n",
      "|816221|          Liveo|@redrobinrockn Yo...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    45|\n",
      "|816222|          alpew|Exeter City take ...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    45|\n",
      "|816223|     mallverine|@30comau I'll che...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    46|\n",
      "|816224|     musicisme7|@taylorswift13 wh...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    46|\n",
      "|816225|       megan901|@muhfukinchico ha...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    47|\n",
      "|816226|        IsJonas|A big mean asshol...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    46|\n",
      "|816227|         patrix|@jinadcruz @keith...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    47|\n",
      "|816228|       Vielfras|Whoa, look what I...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    48|\n",
      "|816229|     bekahkirby|Hiking grandfathe...|  Apr| 18|2009|    Sat|     PDT|  08|    51|    47|\n",
      "+------+---------------+--------------------+-----+---+----+-------+--------+----+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check changes \n",
    "tweet_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e000de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis \n",
    "\n",
    "#!pip install vaderSentiment\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8aeedad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import nltk\n",
    "\n",
    "# Import the lexicon \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "098b0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function  and create sentiment_score column\n",
    "def analyse_sentiment(body):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyser.polarity_scores(body)\n",
    "    return sentiment['compound']\n",
    "\n",
    "sentiment_udf = udf(analyse_sentiment, DoubleType())\n",
    "\n",
    "tweet_data = tweet_data.withColumn(\"sentiment_score\", sentiment_udf(tweet_data['body']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5f8ba3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tweet_data.write.mode(\"overwrite\").saveAsTable(\"tweet_data2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e60e0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column polarity based on compound value\n",
    "tweet_data = tweet_data.withColumn(\"polarity\", when (tweet_data[\"sentiment_score\"]>= 0.05, \"positive\")\n",
    "                                                .when(tweet_data[\"sentiment_score\"]<=-0.05, \"negative\")\n",
    "                                                .otherwise(\"neutral\")\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "702acc48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+--------------------------------------------------------------------------------------------------------+-----+---+----+-------+--------+----+------+------+---------------+--------+\n",
      "|id    |name           |body                                                                                                    |month|day|year|weekday|timezone|hour|minute|second|sentiment_score|polarity|\n",
      "+------+---------------+--------------------------------------------------------------------------------------------------------+-----+---+----+-------+--------+----+------+------+---------------+--------+\n",
      "|816210|prosario_2000  |@ctribe I hope you are having a great day.                                                              |Apr  |18 |2009|Sat    |PDT     |08  |51    |40    |0.7906         |positive|\n",
      "|816211|Chelsea_Volturi|@Boy_Kill_Boy Nope Just Bored Well Say That Most Of The Time The Usual                                  |Apr  |18 |2009|Sat    |PDT     |08  |51    |39    |0.0            |neutral |\n",
      "|816212|askbillmitchell|@marty0518 Sometimes? and just a little cryptic? LOL! I am just messing with you.  You are a good sport.|Apr  |18 |2009|Sat    |PDT     |08  |51    |41    |0.7787         |positive|\n",
      "|816213|kendiixd       |so i guesss im not in coolifornia anymore how exiting                                                   |Apr  |18 |2009|Sat    |PDT     |08  |51    |41    |0.0            |neutral |\n",
      "|816214|ladycalypso    |@DaiLS I do that, too, but right now, it's the Radiant Dawn Soundtrack.                                 |Apr  |18 |2009|Sat    |PDT     |08  |51    |42    |0.631          |positive|\n",
      "+------+---------------+--------------------------------------------------------------------------------------------------------+-----+---+----+-------+--------+----+------+------+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display data\n",
    "tweet_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eff807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
